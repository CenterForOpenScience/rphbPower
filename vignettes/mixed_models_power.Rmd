---
title: "Mixed Models Power Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Mixed Models Power Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(rphbPower)
```

Power analysis for mixed effects models using partial correlations within the unified framework with design effects, ICC adjustments, and hierarchical data structures.

## Overview

Mixed effects models analyze hierarchical/clustered data by accounting for both fixed effects (population relationships) and random effects (group variation). This extends the unified framework to multilevel structures while maintaining partial correlation consistency across all analytical approaches.

## Key Framework Features

- **Unified Effect Size Metric**: All effects use partial correlations for direct comparison
- **Automatic Parameter Detection**: Provide any two of (r_partial, n_groups, power) - the third is calculated
- **Conservative Planning**: Built-in discount factor (default 0.75) for realistic estimates
- **Framework Integration**: Seamless conversion from Cohen's d, f², R² with design effect adjustments
- **Multi-Level Analysis**: Separate handling for Level-1 and Level-2 effects

## Quick Start

```r
library(rphbPower)

# Calculate power (provide effect size and design)
mixed_models_power(r_partial = 0.25, n_groups = 25, n_per_group = 20, icc = 0.08)

# Calculate sample size (provide effect size and power)
mixed_models_power(r_partial = 0.20, power = 0.8, n_per_group = 15, icc = 0.05)

# Calculate effect size (provide design and power)
mixed_models_power(n_groups = 30, power = 0.8, n_per_group = 12, icc = 0.15)

# Framework integration (automatic conversion and design effects)
mixed_models_framework_power(effect_size = 0.5, effect_type = "d", power = 0.8, n_per_group = 20, icc = 0.10)
```

## Main Functions

### `mixed_models_power()`
Core power analysis with framework integration.

**Parameters:**

- `r_partial`: Partial correlation coefficient (NULL to calculate)
- `n_groups`: Number of groups/clusters (NULL to calculate)
- `n_per_group`: Average observations per group (default = 10)
- `power`: Statistical power (NULL to calculate, default = 0.8)
- `icc`: Intraclass correlation coefficient (default = 0.05)
- `alpha`: Significance level (default = 0.05)
- `discount_factor`: Conservative discount factor (default = 0.75)
- `test_level`: Level being tested ("level1" or "level2")
- `effect_input`: Raw effect size input (alternative to r_partial)
- `effect_type`: Type of effect_input ("r", "d", "f2", "r_squared", "eta_squared")

### `mixed_models_framework_power()`
Framework-integrated analysis with automatic conversion.

**Parameters:**

- `effect_size`: Effect size value
- `effect_type`: Type of effect size ("r", "d", "f2", "r_squared", "eta_squared")
- `n_groups`: Number of groups (NULL to calculate)
- `power`: Target power (default = 0.8)
- `n_per_group`: Observations per group
- `icc`: Intraclass correlation
- `test_level`: Level being tested

### Convenience Functions

- `mixed_models_sample_size()`: Quick sample size calculation
- `mixed_models_power_check()`: Quick power calculation

## Framework Integration Examples

### Multiple Effect Size Types
```r
# Cohen's d to framework mixed models
mixed_models_framework_power(effect_size = 0.5, effect_type = "d", power = 0.8, n_per_group = 15, icc = 0.08)

# f² to framework mixed models
mixed_models_framework_power(effect_size = 0.15, effect_type = "f2", n_groups = 30, n_per_group = 20, icc = 0.10)

# Direct partial correlation
mixed_models_power(r_partial = 0.25, power = 0.8, n_per_group = 18, icc = 0.12)
```

### Literature Integration
```r
# Convert mixed literature effects to unified framework
effect_sizes <- c(0.25, 0.4, 0.12)    # r, Cohen's d, f²
effect_types <- c("r", "d", "f2")

# Framework conversion workflow using individual conversions
framework_rs <- sapply(1:length(effect_sizes), function(i) {
  framework_effect_size(effect_sizes[i], effect_types[i], apply_discount = TRUE)
})
```

## Effect Size Guidelines

### Framework Interpretation (Cohen's Standard)

- **r < 0.10**: Negligible effect
- **r = 0.10-0.29**: Small effect
- **r = 0.30-0.49**: Medium effect
- **r ≥ 0.50**: Large effect

### Field-Specific Planning (Cohen's d equivalents)

- **Education**: d = 0.3 (r ≈ 0.11 after discount), ICC = 0.10-0.20
- **Health**: d = 0.5 (r ≈ 0.19 after discount), ICC = 0.02-0.08
- **Psychology**: d = 0.4 (r ≈ 0.15 after discount), ICC = 0.05-0.15
- **Organizational**: d = 0.35 (r ≈ 0.13 after discount), ICC = 0.10-0.25

## Design Effects and Clustering

### Intraclass Correlation (ICC)
Represents proportion of variance due to between-group differences:

- **ICC < 0.05**: Low clustering
- **ICC = 0.05-0.15**: Moderate clustering
- **ICC > 0.15**: High clustering

### Design Effect Impact
```r
# Design Effect = 1 + (n_per_group - 1) × ICC
# Low clustering (ICC = 0.02, 20 per group): Design Effect = 1.38
# Moderate clustering (ICC = 0.10, 20 per group): Design Effect = 2.90
# High clustering (ICC = 0.20, 20 per group): Design Effect = 4.80
```

## Sample Size Planning

### Framework-Based Planning Approach

Sample size requirements for mixed models depend heavily on effect size, ICC, cluster size, and level being tested. Rather than providing potentially misleading rules of thumb, use the framework's power analysis functions to determine precise requirements for your specific design:

```r
# Systematic planning workflow
# Step 1: Define your effect size and study design
effect_size <- framework_effect_size(0.4, "d", apply_discount = TRUE)  # Convert & discount
study_icc <- 0.10  # Based on similar studies in your field
cluster_size <- 20  # Observations per group

# Step 2: Calculate required number of groups
sample_result <- mixed_models_power(
  r_partial = effect_size, 
  power = 0.8, 
  n_per_group = cluster_size, 
  icc = study_icc,
  test_level = "level1"
)

# Step 3: Assess power with available resources  
power_result <- mixed_models_power(
  r_partial = effect_size,
  n_groups = 25,  # Available groups
  n_per_group = cluster_size,
  icc = study_icc,
  test_level = "level1"
)
```

### Planning Considerations by Study Type

**Educational Interventions** (typical ICC = 0.10-0.20):
```r
# School-based intervention planning
mixed_models_power(r_partial = 0.15, power = 0.8, n_per_group = 25, icc = 0.15, test_level = "level1")
```

**Health Outcomes** (typical ICC = 0.02-0.08):
```r
# Clinic-based study planning  
mixed_models_power(r_partial = 0.20, power = 0.8, n_per_group = 30, icc = 0.05, test_level = "level1")
```

**Organizational Research** (typical ICC = 0.10-0.25):
```r
# Workplace intervention planning
mixed_models_power(r_partial = 0.18, power = 0.8, n_per_group = 15, icc = 0.20, test_level = "level1")
```

### Level-1 vs Level-2 Effects
```r
# Level-1 effects (affected by clustering)
mixed_models_power(r_partial = 0.25, power = 0.8, n_per_group = 15, icc = 0.10, test_level = "level1")

# Level-2 effects (less affected by clustering)
mixed_models_power(r_partial = 0.25, power = 0.8, n_per_group = 15, icc = 0.10, test_level = "level2")
```

## Best Practices

### Framework Workflow

1. **Use framework functions** for all effect size conversions
2. **Apply default discount factor** (0.75) for conservative planning
3. **Leverage auto-detection** by providing any 2 of 3 parameters
4. **Account for design effects** through ICC specification

### Study Planning

1. **Literature Integration**: Convert all effect sizes to partial correlations
2. **ICC Estimation**: Use realistic ICC values from similar studies in your field
3. **Level Specification**: Clearly distinguish Level-1 vs Level-2 effects
4. **Design Effect Planning**: Account for substantial efficiency loss from clustering
5. **Framework-Based Calculations**: Use power analysis functions rather than rules of thumb for precise planning

### Quality Assurance

1. **Framework Validation**: All inputs validated by core utilities
2. **Mathematical Accuracy**: Proper design effect and ICC calculations
3. **Consistent Interpretation**: Standard Cohen's effect size guidelines
4. **Cross-method Comparison**: Direct comparison with single-level regression results

## Integration with Unified Framework

### Framework Functions Used

- `framework_effect_size()`: Unified effect size conversion
- `validate_partial_r()`: Input validation
- `effective_sample_size()`: Design effect adjustments
- `framework_conversion_summary()`: Complete effect size reports

### Package Integration
This mixed models analysis integrates seamlessly with all other framework methods:

- **Linear Regression**: Level-1 effects equivalent to single-level regression with clustering penalty
- **Correlation Analysis**: Simple Level-1 effects reduce to correlation with design effects
- **Mediation Analysis**: Multilevel mediation using same partial correlation metrics
- **Logistic Regression**: Extension to multilevel generalized linear models

Mixed models demonstrate the unified framework's scalability to complex hierarchical structures while maintaining mathematical consistency and interpretive clarity through shared partial correlation metrics, enabling researchers to understand clustering effects and design penalties within the broader analytical ecosystem.

## Examples

Essential scenarios demonstrating mixed models power analysis within the unified framework using partial correlations with ICC and design effects.

Author: Power Analysis Package
Version: 1.2

### EXAMPLE 1: BASIC FRAMEWORK WORKFLOW

```{r ex1}
# Research: Individual-level effect in clustered data (students in schools)
# Expected partial r = 0.25, 20 schools, 25 students per school, ICC = 0.08

result1 <- mixed_models_power(r_partial = 0.25, n_groups = 20, n_per_group = 25, 
                              icc = 0.08, test_level = "level1")
print(result1)
```

### EXAMPLE 2: SAMPLE SIZE PLANNING WITH FRAMEWORK

```{r ex2}
# Target: 80% power to detect level-1 effect
# Expected r = 0.20, 15 per cluster, ICC = 0.05

result2 <- mixed_models_power(r_partial = 0.20, power = 0.8, n_per_group = 15, 
                              icc = 0.05, test_level = "level1")

cat("Groups needed:", result2$n_groups, ", Total n:", result2$total_n, 
    ", Effective n:", result2$effective_n, "\n")
```

### EXAMPLE 3: ICC IMPACT COMPARISON

```{r ex3}
# Fixed effect size, varying ICC values
# Demonstrates design effect impact on sample requirements

r_fixed <- 0.25
n_per_group_fixed <- 15

cat("ICC impact on required groups (r = 0.25, 15 per group):\n")
for (icc_val in c(0.01, 0.05, 0.10, 0.20)) {
  n_groups_needed <- mixed_models_sample_size(r_partial = r_fixed, power = 0.8, 
                                              n_per_group = n_per_group_fixed, 
                                              icc = icc_val, test_level = "level1")
  design_eff <- 1 + (n_per_group_fixed - 1) * icc_val
  cat("ICC =", icc_val, ": ", n_groups_needed, "groups, Design Effect =", round(design_eff, 2), "\n")
}

# Shows dramatic sample size increases with higher ICC values
```

### EXAMPLE 4: FRAMEWORK EFFECT SIZE CONVERSIONS

```{r ex4}
# Convert Cohen's d = 0.5 to framework mixed models analysis

result4 <- mixed_models_framework_power(
  effect_size = 0.5,
  effect_type = "d",
  power = 0.8,
  n_per_group = 20,
  icc = 0.10,
  test_level = "level1"
)

cat("Cohen's d = 0.5 → Framework r =", round(result4$r_partial, 3),
    ", Required groups:", result4$n_groups, "\n")
```

### EXAMPLE 5: LEVEL-1 VS LEVEL-2 COMPARISON

```{r ex5}
# Compare power for different effect levels with same design

effect_r <- 0.30
groups <- 25
per_group <- 10
icc_val <- 0.12

level1_power <- mixed_models_power_check(r_partial = effect_r, n_groups = groups, 
                                         n_per_group = per_group, icc = icc_val, 
                                         test_level = "level1")

level2_power <- mixed_models_power_check(r_partial = effect_r, n_groups = groups, 
                                         n_per_group = per_group, icc = icc_val, 
                                         test_level = "level2")

cat("Level comparison (r = 0.30, 25 groups, 10 per group, ICC = 0.12):\n")
cat("Level-1 power:", round(level1_power, 3), ", Level-2 power:", round(level2_power, 3), "\n")

# Demonstrates different power characteristics for individual vs. group-level effects
```

### EXAMPLE 6: DETECTABLE EFFECT SIZE ANALYSIS

```{r ex6}
# Study constraints: 30 groups, 12 per group, ICC = 0.15, want 80% power

result6 <- mixed_models_power(n_groups = 30, power = 0.8, n_per_group = 12, 
                              icc = 0.15, test_level = "level1")

cat("With 30 groups, 12 per group, ICC = 0.15, 80% power detects r =", 
    round(result6$r_partial, 3), "\n")
```

### EXAMPLE 7: FIELD-SPECIFIC APPLICATIONS

```{r ex7}
# Typical effect sizes by research field using Cohen's d equivalents

fields <- data.frame(
  Field = c("Education", "Health", "Psychology", "Organizational"),
  Effect_d = c(0.3, 0.5, 0.4, 0.35),
  Per_group = c(25, 15, 20, 12),
  ICC = c(0.12, 0.05, 0.08, 0.15)
)

cat("Sample sizes for 80% power by field:\n")
for (i in 1:nrow(fields)) {
  field_result <- mixed_models_framework_power(
    effect_size = fields$Effect_d[i],
    effect_type = "d",
    power = 0.8,
    n_per_group = fields$Per_group[i],
    icc = fields$ICC[i],
    test_level = "level1"
  )
  cat(fields$Field[i], "(d =", fields$Effect_d[i], "):", 
      field_result$n_groups, "groups\n")
}

# Provides field-specific guidance for clustered designs
```

### EXAMPLE 8: DESIGN OPTIMIZATION

```{r ex8}
# Compare different design configurations with same total sample size

total_n <- 400
comparison_r <- 0.20
comparison_icc <- 0.08

designs <- data.frame(
  n_groups = c(20, 40, 50),
  n_per_group = c(20, 10, 8)
)

cat("Design comparison (Total N = 400, r = 0.20, ICC = 0.08):\n")
for (i in 1:nrow(designs)) {
  power_result <- mixed_models_power_check(r_partial = comparison_r, 
                                           n_groups = designs$n_groups[i], 
                                           n_per_group = designs$n_per_group[i], 
                                           icc = comparison_icc, 
                                           test_level = "level1")
  
  cat(designs$n_groups[i], "groups ×", designs$n_per_group[i], "per group: Power =", 
      round(power_result, 3), "\n")
}

# Demonstrates optimal design trade-offs between groups and group size
```

### EXAMPLE 9: FRAMEWORK CONVERSION DEMONSTRATION

```{r ex9}
# Demonstrate framework conversions for mixed models

effect_sizes <- c(0.20, 0.30, 0.40)  # Partial correlations

cat("Framework conversions for mixed models:\n")
for (r in effect_sizes) {
  result <- mixed_models_power(r_partial = r, power = 0.8, n_per_group = 15, 
                               icc = 0.08, test_level = "level1")
  cat("Input r =", r, ": d =", round(result$effect_size_conversions$cohens_d, 3), 
      ", R² =", round(result$effect_size_conversions$r_squared, 3), 
      ", Groups needed =", result$n_groups, "\n")
}
```

### EXAMPLE 10: CROSS-METHOD FRAMEWORK CONSISTENCY

```{r ex10}
# Compare mixed models with equivalent single-level analysis

level1_effect <- 0.25
icc_value <- 0.10
n_per_cluster <- 15

# Mixed models analysis (Level-1 effect)
mixed_result <- mixed_models_power(r_partial = level1_effect, power = 0.8, 
                                   n_per_group = n_per_cluster, icc = icc_value, 
                                   test_level = "level1")

# Calculate design effect penalty
design_effect <- 1 + (n_per_cluster - 1) * icc_value
single_level_equivalent <- round(mixed_result$effective_n)

cat("Cross-method comparison for r = 0.25:\n")
cat("Mixed models (ICC = 0.10):", mixed_result$total_n, "total participants\n")
cat("Effective sample size:", single_level_equivalent, "participants\n")
cat("Design effect penalty:", round(design_effect, 2), "x\n")

# Illustrates clustering impact on effective sample size
```

### EXAMPLE 11: LITERATURE INTEGRATION WORKFLOW

```{r ex11}
# Step 1: Literature review with mixed effect sizes

literature_effects <- data.frame(
  Study = c("A", "B", "C"),
  Effect = c(0.25, 0.4, 0.12),  # r, Cohen's d, f²
  Type = c("r", "d", "f2"),
  ICC = c(0.08, 0.12, 0.05),
  Per_group = c(20, 15, 25)
)

# Step 2: Convert to framework partial correlations using effect_input parameter
framework_results <- list()
for (i in 1:nrow(literature_effects)) {
  framework_results[[i]] <- mixed_models_power(
    effect_input = literature_effects$Effect[i], 
    effect_type = literature_effects$Type[i],
    power = 0.8,
    n_per_group = literature_effects$Per_group[i],
    icc = literature_effects$ICC[i],
    test_level = "level1"
  )
}

# Step 3: Meta-analytic planning
framework_rs <- sapply(framework_results, function(x) x$r_partial)
meta_r <- mean(framework_rs)
meta_icc <- mean(literature_effects$ICC)
meta_per_group <- round(mean(literature_effects$Per_group))

meta_result <- mixed_models_power(r_partial = meta_r, power = 0.8, 
                                  n_per_group = meta_per_group, icc = meta_icc, 
                                  test_level = "level1")

cat("Literature integration: Meta r =", round(meta_r, 3),
    ", Required groups =", meta_result$n_groups, "\n")
```

### EXAMPLE 12: SENSITIVITY ANALYSIS

```{r ex12}
# Test robustness to ICC uncertainty

base_r <- 0.25
n_per_group <- 20

# Test ICC variation around estimated value
icc_variations <- c(0.03, 0.05, 0.08, 0.12, 0.15)

cat("Sample size sensitivity to ICC (r = 0.25, 20 per group, 80% power):\n")
for (icc in icc_variations) {
  sens_result <- mixed_models_power(r_partial = base_r, power = 0.8, 
                                    n_per_group = n_per_group, icc = icc, 
                                    test_level = "level1")
  cat("ICC =", icc, ": ", sens_result$n_groups, "groups (total n =", sens_result$total_n, ")\n")
}

# Evaluates planning robustness to clustering assumptions
```

## FRAMEWORK BEST PRACTICES

- Use mixed_models_framework_power() for automatic effect size conversion
- Higher ICC dramatically increases sample size requirements through design effects
- Level-2 effects require fewer total participants but more clusters
- Framework enables direct comparison with single-level regression results
- Design effect = 1 + (n_per_group - 1) × ICC quantifies clustering impact
- Consider sensitivity analysis for ICC uncertainty in planning
- Balance number of groups vs. group size based on research priorities
