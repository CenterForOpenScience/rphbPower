---
title: "Fixed Effects Power Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fixed Effects Power Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all()
```

Power analysis for fixed effects models using partial correlations within the unified framework with integrated design effects and conservative planning for panel data.

## Overview

Fixed effects models examine within-person relationships by controlling for all time-invariant individual differences, providing strong causal inference from panel data. All effect sizes are converted to partial correlations for direct comparison across correlation, regression, mediation, SEM, and other framework analyses.

## Key Framework Features

- **Unified Effect Size Metric**: All analyses use partial correlations for direct comparison
- **Automatic Parameter Detection**: Provide any two of (r_partial, n_units, power) - the third is calculated
- **Conservative Planning**: Built-in discount factor (default 0.75) for realistic estimates
- **Framework Integration**: Seamless conversion from Cohen's d, f², R², eta-squared
- **Design Effects**: Accounts for clustering from multiple observations per person

## Quick Start

```{r setup}
library(rphbPower)
```

```r
# Calculate power (provide r_partial and n_units)
fixed_effects_power(r_partial = 0.15, n_units = 150, n_periods = 8)

# Calculate sample size (provide r_partial and power)
fixed_effects_power(r_partial = 0.12, power = 0.8, n_periods = 6)

# Calculate effect size (provide n_units and power)
fixed_effects_power(n_units = 200, power = 0.8, n_periods = 10)

# Framework integration (automatic conversion and discount)
fixed_effects_framework_power(effect_size = 0.4, effect_type = "d", power = 0.8, n_periods = 6)
```

## Main Functions

### `fixed_effects_power()`
Core power analysis with framework integration.

**Parameters:**

- `r_partial`: Within-person partial correlation coefficient (NULL to calculate)
- `n_units`: Number of individuals/units (NULL to calculate)
- `n_periods`: Number of time periods per unit (default = 4)
- `power`: Statistical power (NULL to calculate, default = 0.8)
- `icc`: Intraclass correlation coefficient (default = 0.3)
- `alpha`: Significance level (default = 0.05)
- `discount_factor`: Conservative discount factor (default = 0.75)
- `effect_input`: Raw effect size input (alternative to r_partial)
- `effect_type`: Type of effect_input ("r", "d", "f2", "r_squared", "eta_squared")

### `fixed_effects_framework_power()`
Framework-integrated analysis with automatic conversion.

**Parameters:**

- `effect_size`: Effect size value
- `effect_type`: Type of effect size ("r", "d", "f2", "r_squared", "eta_squared")
- `n_units`: Number of units (NULL to calculate)
- `power`: Target power (default = 0.8)
- `n_periods`: Number of periods

### Convenience Functions

- `fixed_effects_sample_size()`: Quick sample size calculation
- `fixed_effects_power_check()`: Quick power calculation

## Framework Integration Examples

### Multiple Effect Size Types
```r
# Cohen's d to framework fixed effects
fixed_effects_framework_power(effect_size = 0.4, effect_type = "d", power = 0.8, n_periods = 6)

# R² to framework fixed effects
fixed_effects_framework_power(effect_size = 0.06, effect_type = "r_squared", n_units = 150, n_periods = 8)

# Direct partial correlation
fixed_effects_power(r_partial = 0.18, power = 0.8, n_periods = 6)
```

### Cross-Method Efficiency Comparison
```r
# Same effect size: fixed effects vs. cross-sectional regression
effect_r <- 0.18

# Fixed effects (within-person, controls all time-invariant confounds)
fe_result <- fixed_effects_power(r_partial = effect_r, power = 0.8, n_periods = 6, icc = 0.3)

# Cross-sectional regression (framework comparison)
source("05_methods/5.2_regression/linear_regression/linear_regression_power_analysis.R")
cs_result <- linear_regression_power(r_partial = effect_r, power = 0.8, n_predictors = 1)

# Efficiency comparison: fe_result$n_units vs cs_result$n
```

## Effect Size Guidelines

### Framework Interpretation (Cohen's Standard)

- **r < 0.10**: Negligible within-person effect
- **r = 0.10-0.29**: Small within-person effect
- **r = 0.30-0.49**: Medium within-person effect
- **r ≥ 0.50**: Large within-person effect

### ICC Context Adjustment

**High ICC (0.5+)**: Small effects (r = 0.05-0.15) more meaningful due to limited within-person variation

**Low ICC (<0.3)**: Standard interpretation applies for within-person effects

### Field-Specific Planning (Within-person effects)

- **Psychology**: r = 0.12-0.20 (daily mood-behavior relationships)
- **Education**: r = 0.08-0.18 (weekly study-performance cycles)
- **Health**: r = 0.15-0.25 (daily symptoms-activity relationships)
- **Organizational**: r = 0.10-0.22 (weekly workload-satisfaction dynamics)

## Sample Size Planning

Fixed effects models require careful consideration of ICC, number of periods, and within-person effect sizes. Use framework power analysis functions to determine precise requirements:

```r
# Assess ICC impact on sample size requirements
fixed_effects_power(r_partial = 0.15, power = 0.8, n_periods = 6, icc = 0.2)
fixed_effects_power(r_partial = 0.15, power = 0.8, n_periods = 6, icc = 0.4)
fixed_effects_power(r_partial = 0.15, power = 0.8, n_periods = 6, icc = 0.6)

# Evaluate periods vs sample size trade-offs
fixed_effects_power(r_partial = 0.15, power = 0.8, n_periods = 4, icc = 0.35)
fixed_effects_power(r_partial = 0.15, power = 0.8, n_periods = 8, icc = 0.35)
fixed_effects_power(r_partial = 0.15, power = 0.8, n_periods = 12, icc = 0.35)
```

## Best Practices

### Framework Workflow

1. **Use framework functions** for all effect size conversions
2. **Apply default discount factor** (0.75) for conservative planning
3. **Leverage auto-detection** by providing any 2 of 3 parameters
4. **Check framework conversions** in results for cross-method comparison

### Study Planning

1. **Estimate ICC** from pilot data or similar studies in your field
2. **Account for attrition** in longitudinal designs (plan 20-30% larger initial sample)
3. **Balance periods vs. burden** considering participant retention
4. **Focus on within-person variation** for causal interpretation

### Quality Assurance

1. **Framework Validation**: All inputs validated by core utilities
2. **Mathematical Accuracy**: Base R calculations ensure reliability
3. **Consistent Interpretation**: Standard Cohen's effect size guidelines
4. **Cross-method Comparison**: Direct comparison with regression, repeated measures, mediation results

## Integration with Unified Framework

### Framework Functions Used

- `framework_effect_size()`: Unified effect size conversion
- `validate_partial_r()`: Input validation
- `framework_conversion_summary()`: Complete effect size report
- `partial_r_to_cohens_f2()`: Framework-consistent conversions

### Package Integration
This fixed effects analysis integrates seamlessly with all other framework methods:

- **Linear Regression**: Same r_partial values for cross-sectional comparisons
- **Repeated Measures**: Consistent effect size metric for temporal designs
- **Cross-Lagged Panel**: Framework foundation for reciprocal causation models
- **Mixed Models**: Unified effect interpretation across clustering approaches

Fixed effects models provide the strongest causal inference within the unified framework, with all within-person analyses building on regression principles while maintaining mathematical consistency and interpretive clarity through shared partial correlation metrics that inherently control for all time-invariant confounding.

## Examples

Essential scenarios demonstrating fixed effects power analysis within the unified framework using partial correlations and ICC considerations.

Author: Power Analysis Package
Version: 1.2

### EXAMPLE 1: BASIC FRAMEWORK WORKFLOW

```{r ex1}
# Research: Within-person effect of daily stress on performance
# Expected within-person effect r = 0.20, moderate ICC

result1 <- fixed_effects_power(
  r_partial = 0.20,
  power = 0.8,
  n_periods = 14,
  icc = 0.35
)
print.fixed_effects_power_analysis(result1)
```

### EXAMPLE 2: FRAMEWORK EFFECT SIZE CONVERSION

```{r ex2}
# Literature reports Cohen's d = 0.4 for within-person intervention effects

result2 <- fixed_effects_power(
  effect_input = 0.4,
  effect_type = "d",
  power = 0.8,
  n_periods = 8,
  icc = 0.3
)

cat("Cohen's d = 0.4 → Framework r =", round(result2$r_partial, 3), 
    ", Required n =", result2$n_units, "\n")
```

### EXAMPLE 3: ICC IMPACT ON SAMPLE SIZE

```{r ex3}
# How clustering affects sample size requirements

base_effect <- 0.15
icc_levels <- c(0.2, 0.4, 0.6)

cat("Sample size by ICC level (r = 0.15, 6 periods):\n")
for (icc in icc_levels) {
  result <- fixed_effects_power(r_partial = base_effect, power = 0.8, 
                                n_periods = 6, icc = icc)
  cat("ICC =", icc, ": n =", result$n_units, 
      " (design effect =", round(result$design_effect, 2), ")\n")
}
```

### EXAMPLE 4: CROSS-METHOD EFFICIENCY COMPARISON

```{r ex4}
# Same effect size: fixed effects vs. cross-sectional regression

effect_r <- 0.18

# Fixed effects (within-person)
fe_result <- fixed_effects_power(r_partial = effect_r, power = 0.8, 
                                 n_periods = 6, icc = 0.3)

# Cross-sectional comparison (simplified - using approximate multiplier based on design effect)
cs_approximate_n <- round(fe_result$n_units / fe_result$design_effect)

cat("Sample size comparison for r =", effect_r, ":\n")
cat("Fixed effects:", fe_result$n_units, "vs Cross-sectional (approx.):", cs_approximate_n, "\n")
cat("Panel cost:", round(fe_result$n_units / cs_approximate_n, 1), "x\n")
```

### EXAMPLE 5: TIME PERIODS PLANNING

```{r ex5}
# Balancing measurement burden with statistical power

period_scenarios <- c(4, 8, 12, 16)
effect_size <- 0.16

cat("Sample sizes for r = 0.16, ICC = 0.35:\n")
for (periods in period_scenarios) {
  result <- fixed_effects_power(r_partial = effect_size, power = 0.8, 
                                n_periods = periods, icc = 0.35)
  cat(periods, "periods:", result$n_units, "units\n")
}
```

### EXAMPLE 6: FIELD-SPECIFIC APPLICATIONS

```{r ex6}
# Typical effect sizes and designs by research area

fields <- data.frame(
  Field = c("Psychology", "Education", "Health", "Organizational"),
  Effect_r = c(0.15, 0.12, 0.20, 0.18),
  Periods = c(21, 10, 14, 8),
  ICC = c(0.35, 0.45, 0.25, 0.40)
)

cat("Sample sizes for 80% power by field:\n")
for (i in 1:nrow(fields)) {
  result <- fixed_effects_power(
    r_partial = fields$Effect_r[i],
    power = 0.8,
    n_periods = fields$Periods[i],
    icc = fields$ICC[i]
  )
  cat(fields$Field[i], "(r =", fields$Effect_r[i], ",", fields$Periods[i], "periods):", result$n_units, "\n")
}
```

### EXAMPLE 7: PILOT TO MAIN STUDY SCALING

```{r ex7}
# Conservative planning from pilot data

pilot_effect <- 0.28  # Optimistic pilot result
conservative_planning <- fixed_effects_power(
  effect_input = pilot_effect,
  effect_type = "r",  # Framework applies 0.75 discount automatically
  power = 0.8,
  n_periods = 7,
  icc = 0.30
)

cat("Pilot effect r =", pilot_effect, "→ Planning r =", 
    round(conservative_planning$r_partial, 3), ", Required n =", conservative_planning$n_units, "\n")
```

### EXAMPLE 8: DETECTABLE EFFECT SIZE ANALYSIS

```{r ex8}
# Resource-constrained study: what can we detect?

constrained_n <- 150
periods <- 8

detectable_result <- fixed_effects_power(
  n_units = constrained_n,
  power = 0.8,
  n_periods = periods,
  icc = 0.35
)

cat("With n =", constrained_n, ", can detect within-person r =", 
    round(detectable_result$r_partial, 3), "\n")
```

### EXAMPLE 9: FRAMEWORK CONVERSION TABLE

```{r ex9}
# Multiple effect sizes for planning comparison

effect_sizes <- c(0.12, 0.18, 0.24)

for (r in effect_sizes) {
  result <- fixed_effects_power(r_partial = r, power = 0.8, 
                                n_periods = 6, icc = 0.35)
  cat("r =", r, ": n =", result$n_units, 
      " (d =", round(result$effect_size_conversions$cohens_d, 2), ")\n")
}
```

### EXAMPLE 10: LITERATURE INTEGRATION WORKFLOW

```{r e10}
# Multiple studies with different effect metrics

studies <- data.frame(
  Study = c("A", "B", "C"),
  Effect = c(0.35, 0.06, 0.20),  # d, R², direct r
  Type = c("d", "r_squared", "r"),
  Periods = c(7, 10, 14),
  ICC = c(0.3, 0.4, 0.35)
)

# Convert to framework partial correlations using direct function calls
framework_results <- list()
for (i in 1:nrow(studies)) {
  framework_results[[i]] <- fixed_effects_power(
    effect_input = studies$Effect[i], 
    effect_type = studies$Type[i], 
    power = 0.8,
    n_periods = studies$Periods[i],
    icc = studies$ICC[i]
  )
}

# Meta-analytic planning
framework_effects <- sapply(framework_results, function(x) x$r_partial)
meta_r <- mean(framework_effects)
meta_periods <- round(mean(studies$Periods))
meta_icc <- mean(studies$ICC)

meta_result <- fixed_effects_power(
  r_partial = meta_r,
  power = 0.8,
  n_periods = meta_periods,
  icc = meta_icc
)

cat("Literature integration: Meta r =", round(meta_r, 3), 
    ", Required n =", meta_result$n_units, "\n")
```

## FRAMEWORK BEST PRACTICES

- Use fixed_effects_power() with effect_input for automatic conversion and discount
- Higher ICC dramatically reduces effective sample size via design effects
- Framework partial correlations enable comparison with regression/mediation
- Within-person effects control for all time-invariant confounds
- Conservative planning with 0.75 discount handles optimistic pilot effects
