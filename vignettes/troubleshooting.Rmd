---
title: "Troubleshooting Guide"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Troubleshooting Guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all()
library(rphbPower)
```

Complete diagnostic and solution guide for power analysis issues within the unified partial correlation framework with working setup system and cross-method integration.

## Framework Overview

The unified framework uses **partial correlations** as the standardized effect size metric across all analysis types, with integrated setup system, conservative planning, and cross-method consistency. Most issues arise from setup problems, effect size conversion errors, or unrealistic parameter combinations.

**Core Architecture:**

- **Unified effect sizes**: All analyses use partial correlations for direct comparison
- **Auto-detection**: Provide any 2 of (effect size, sample size, power) - calculates the third
- **Framework integration**: Seamless conversion from Cohen's d, f², R², eta-squared

## Quick Diagnostic Workflow

### Step 1: Test Setup System
```r
# Test if setup system works
source("02_setup.R")

# Should show: "Unified Power Analysis Package Ready - SETUP FIXED!"
# If not, see Setup Issues section below
```

### Step 2: Verify Method Access
```r
# Check available methods
show_available_methods()

# Load specific method
load_analysis_method("linear_regression")

# Test basic functionality
linear_regression_power(r_partial = 0.25, power = 0.8, n_predictors = 1)
```

### Step 3: Framework Quick Start
```r
# All-in-one approach with automatic conversion
framework_quick_start(0.4, "d", "linear_regression", power = 0.8, n_predictors = 2)
```

If Steps 1-3 work, your setup is correct. Skip to **Parameter Issues** section.

## Setup Issues - FIXED SYSTEM

### Setup File Not Loading
```
Error: cannot open file '02_setup.R': No such file or directory
```
**Diagnosis:** Setup file missing or corrupted

**Solution:**
```r
# Verify file exists
file.exists("02_setup.R")

# If FALSE, restore setup file from package
# If TRUE but still errors, check file permissions

# Fallback: Direct loading
source("04_core/compute_partial_correlations.R")
source("05_methods/5.2_regression/linear_regression/linear_regression_power_analysis.R")
```

### Functions Not Found After Setup
```
Error: could not find function "show_available_methods"
```
**Diagnosis:** Setup loaded but functions not created

**Solution:**
```r
# Verify setup success message appeared
source("02_setup.R")
# Should show: "Unified Power Analysis Package Ready - SETUP FIXED!"

# Check function existence
exists("show_available_methods")  # Should be TRUE
exists("load_analysis_method")    # Should be TRUE
exists("framework_quick_start")   # Should be TRUE

# If FALSE, setup system failed - use direct loading fallback
```

### Method Loading Failures
```
Warning: Analysis method 'linear_regression' not available
```
**Diagnosis:** Method files missing or paths incorrect

**Solution:**
```r
# Check what methods are actually available
show_available_methods()

# For missing methods, check file structure
file.exists("05_methods/5.2_regression/linear_regression/linear_regression_power_analysis.R")

# Use available alternative methods
load_analysis_method("correlation")  # Simpler alternative
```

## Parameter Detection Issues

### Wrong Number of Parameters
```
Error: Provide exactly two of: r_partial, n, power (or use effect_input)
```
**Framework pattern:** Every function requires exactly 2 of 3 core parameters

**Common Errors and Solutions:**
```r
# ❌ ERROR: Only 1 parameter
linear_regression_power(r_partial = 0.25)

# ✅ CORRECT: Exactly 2 parameters
linear_regression_power(r_partial = 0.25, power = 0.8, n_predictors = 1)    # → n
linear_regression_power(r_partial = 0.25, n = 120, n_predictors = 1)        # → power  
linear_regression_power(n = 150, power = 0.8, n_predictors = 1)             # → r_partial

# ❌ ERROR: All 3 parameters
linear_regression_power(r_partial = 0.25, n = 120, power = 0.8, n_predictors = 1)

# ✅ ALTERNATIVE: Framework integration
linear_regression_power(effect_input = 0.4, effect_type = "d", power = 0.8, n_predictors = 1)
```

### Parameter Validation Errors
```
Error: Partial correlation must be between -1 and 1 (exclusive)
```
**Common Causes:**

| **Input Error** | **Cause** | **Solution** |
|----------------|-----------|-------------|
| `r_partial = 1.2` | Used Cohen's d directly | `framework_effect_size(1.2, "d")` |
| `r_partial = 25` | Used percentage | `r_partial = 0.25` |
| `r_partial = 0.95` | Unrealistically large | Check literature; use realistic effects |

**Framework Conversion Examples:**
```r
# Convert various effect size types
cohen_d <- 0.5
framework_r <- framework_effect_size(cohen_d, "d", apply_discount = TRUE)

# Literature integration with mixed effect types
literature_effects <- c(0.4, 0.12, 0.08)  # d, R², f²
effect_types <- c("d", "r_squared", "f2")

# Convert all to framework standard
r_values <- mapply(framework_effect_size, literature_effects, effect_types, 
                   MoreArgs = list(apply_discount = TRUE))
```

## Method-Specific Issues

### Linear/Logistic Regression
```r
# ❌ COMMON ERROR: Forgetting n_predictors
linear_regression_power(r_partial = 0.25, power = 0.8)

# ✅ SOLUTION: Always specify n_predictors
linear_regression_power(r_partial = 0.25, power = 0.8, n_predictors = 1)    # Simple regression
linear_regression_power(r_partial = 0.20, power = 0.8, n_predictors = 5)    # Multiple regression

# Framework integration
linear_regression_power(effect_input = 0.15, effect_type = "f2", power = 0.8, n_predictors = 3)
```

### Mediation Analysis
```r
# ❌ COMMON ERROR: Missing path coefficients
mediation_regression_power(r_a = 0.30, power = 0.8)  # Missing r_b

# ✅ SOLUTION: Both paths required
mediation_regression_power(r_a = 0.30, r_b = 0.25, power = 0.8)

# Framework integration for both paths
mediation_regression_power(
  effect_input_a = 0.4,    # Cohen's d for a-path
  effect_input_b = 0.3,    # Cohen's d for b-path  
  effect_type = "d",
  power = 0.8
)

# Alternative: Convert paths separately
r_a <- framework_effect_size(0.4, "d", apply_discount = TRUE)
r_b <- framework_effect_size(0.3, "d", apply_discount = TRUE)
mediation_regression_power(r_a = r_a, r_b = r_b, power = 0.8)
```

### Mixed Models
```r
# ❌ COMMON ERROR: Wrong parameter names
mixed_models_power(r_partial = 0.20, n_clusters = 25, n_per_cluster = 8)  # Wrong parameter name

# ✅ SOLUTION: Correct parameter names and exactly 2 of 3 core parameters
mixed_models_power(r_partial = 0.20, n_per_group = 8, power = 0.8)  # Calculate n_groups

# Framework integration - provide exactly 2 of 3 core parameters
mixed_models_power(
  effect_input = 0.09, 
  effect_type = "r_squared", 
  n_per_group = 10, 
  power = 0.8
)  # Calculate n_groups
```

### SEM and Complex Methods
```r
# Key parameter differences across methods
mediation_sem_power(r_a = 0.25, r_b = 0.30, n_indicators_total = 9, power = 0.8)  # Note: n_indicators_total
cross_lagged_panel_power(r_partial = 0.15, n_waves = 3, stability_coefficient = 0.6, power = 0.8)
fixed_effects_power(r_partial = 0.15, n_units = 100, n_periods = 4, power = 0.8)
```

## Unrealistic Results

### Sample Size Too Large (N > 1000)
**Diagnosis:** Effect size too small for practical detection

**Investigation Workflow:**
```r
# Check effect size magnitude
suspect_result <- linear_regression_power(r_partial = 0.05, power = 0.8, n_predictors = 3)
suspect_result$n  # Likely > 1000

# Compare with field-appropriate effects
psychology_result <- linear_regression_power(r_partial = 0.20, power = 0.8, n_predictors = 3)
education_result <- linear_regression_power(r_partial = 0.25, power = 0.8, n_predictors = 3)
```

**Solutions:**
```r
# Use field-specific effect size guidelines (see effect_size_guidelines.md)
field_effects <- list(
  psychology = 0.20,     # Typical medium effect
  education = 0.25,      # Typical medium effect
  health = 0.15,         # Typical medium effect
  business = 0.18        # Typical medium effect
)

# Apply to your field
realistic_result <- linear_regression_power(
  r_partial = field_effects$psychology, 
  power = 0.8, 
  n_predictors = 3
)

# Literature-based planning
meta_analysis_d <- 0.45  # From systematic review
framework_r <- framework_effect_size(meta_analysis_d, "d", apply_discount = TRUE)
literature_result <- linear_regression_power(r_partial = framework_r, power = 0.8, n_predictors = 3)
```

### Sample Size Too Small (N < 50)
**Diagnosis:** Effect size unrealistically large

**Investigation:**
```r
# Check if effect size is too optimistic
optimistic_result <- correlation_power(r_partial = 0.60, power = 0.8)
optimistic_result$n  # Likely < 50

# Validate against literature
literature_range <- c(0.15, 0.25, 0.35)  # Small, medium, large
sample_sizes <- sapply(literature_range, function(r) {
  correlation_power(r_partial = r, power = 0.8)$n
})

data.frame(
  Effect_Size = literature_range,
  Sample_Size = sample_sizes,
  Interpretation = c("Small", "Medium", "Large")
)
```

**Conservative Planning:**
```r
# Use framework discount for pilot data
pilot_r <- 0.45
conservative_r <- framework_effect_size(pilot_r, "r", apply_discount = TRUE)  # Applies 0.75 discount
conservative_result <- correlation_power(r_partial = conservative_r, power = 0.8)
```

## Cross-Method Integration Issues

### Inconsistent Results Across Methods
**Framework enables direct comparison using same r_partial:**

```r
# Same research question, different statistical approaches
research_effect <- 0.25

# Load multiple methods using setup system
source("02_setup.R")
load_analysis_method("correlation")
load_analysis_method("linear_regression") 
load_analysis_method("repeated_measures")

# Compare sample size requirements - provide exactly 2 of 3 for each
correlation_n <- correlation_power(r_partial = research_effect, power = 0.8)$n
regression_n <- linear_regression_power(r_partial = research_effect, power = 0.8, n_predictors = 1)$n
repeated_n <- repeated_measures_power(r_partial = research_effect, power = 0.8, n_timepoints = 2)$n

# Create comparison table
comparison <- data.frame(
  Method = c("Correlation", "Linear Regression", "Repeated Measures"),
  Sample_Size = c(correlation_n, regression_n, repeated_n),
  Efficiency = c("Baseline", "Similar", "More efficient")
)
print(comparison)
```

### Framework Conversion Validation
```r
# Verify conversions make sense
original_d <- 0.5
conversion_summary <- framework_conversion_summary(original_d, "d", apply_discount = TRUE)
print(conversion_summary)

# Cross-check with direct conversion
manual_r <- cohens_d_to_partial_r(original_d, apply_discount = TRUE)
framework_r <- framework_effect_size(original_d, "d", apply_discount = TRUE)

# Should be identical
identical(manual_r, framework_r)  # Should be TRUE
```

## Literature Integration Troubleshooting

### Mixed Effect Size Types
**Problem:** Literature has different effect size metrics

**Framework Solution:**
```r
# Step 1: Organize literature effects
literature_studies <- data.frame(
  Study = c("A", "B", "C", "D"),
  Effect = c(0.4, 0.12, 0.08, 0.25),
  Type = c("d", "r_squared", "f2", "r"),
  Sample_Size = c(120, 200, 180, 150)
)

# Step 2: Convert all to framework standard
framework_effects <- sapply(1:nrow(literature_studies), function(i) {
  framework_effect_size(
    literature_studies$Effect[i], 
    literature_studies$Type[i], 
    apply_discount = TRUE
  )
})

# Step 3: Meta-analytic planning  
meta_r <- mean(framework_effects)
meta_result <- linear_regression_power(r_partial = meta_r, power = 0.8, n_predictors = 3)

# Comprehensive summary
data.frame(
  literature_studies,
  Framework_r = round(framework_effects, 3),
  Meta_Average = round(meta_r, 3)
)
```

### Integration Across Analysis Types
```r
# Use literature effect in multiple analyses
literature_effect <- 0.35  # From meta-analysis
framework_r <- framework_effect_size(literature_effect, "r", apply_discount = TRUE)

# Apply to different research questions
correlation_study <- correlation_power(r_partial = framework_r, power = 0.8)
regression_study <- linear_regression_power(r_partial = framework_r, power = 0.8, n_predictors = 4)
mediation_study <- mediation_regression_power(r_a = framework_r, r_b = framework_r, power = 0.8)

# Compare requirements
study_comparison <- data.frame(
  Analysis = c("Correlation", "Regression", "Mediation"),
  Sample_Size = c(correlation_study$n, regression_study$n, mediation_study$n),
  Complexity = c("Simple", "Moderate", "Complex")
)
```

## Advanced Troubleshooting

### Framework Validation Workflow
```r
# Complete validation check
source("02_setup.R")

# Test 1: Setup system working
setup_working <- exists("load_analysis_method") && exists("show_available_methods")

# Test 2: Method loading
load_analysis_method("linear_regression")
method_working <- exists("linear_regression_power")

# Test 3: Framework integration
test_result <- framework_quick_start(0.25, "r", "linear_regression", power = 0.8, n_predictors = 1)
framework_working <- is.list(test_result) && "n" %in% names(test_result)

# Test 4: Cross-method consistency
load_analysis_method("correlation")
corr_n <- correlation_power(r_partial = 0.25, power = 0.8)$n
reg_n <- linear_regression_power(r_partial = 0.25, power = 0.8, n_predictors = 1)$n
consistency_check <- abs(corr_n - reg_n) < 10  # Should be very similar

# Validation summary
validation_results <- data.frame(
  Test = c("Setup System", "Method Loading", "Framework Integration", "Cross-Method Consistency"),
  Status = c(setup_working, method_working, framework_working, consistency_check)
)
print(validation_results)
```

### Performance Optimization
```r
# For repeated analyses, load all methods once
source("02_setup.R")
loaded_methods <- load_all_methods()

# Batch processing example
effect_sizes <- seq(0.15, 0.35, 0.05)
sample_sizes <- sapply(effect_sizes, function(r) {
  linear_regression_power(r_partial = r, power = 0.8, n_predictors = 3)$n
})

results_table <- data.frame(
  Effect_Size = effect_sizes,
  Sample_Size = sample_sizes,
  Interpretation = sapply(effect_sizes, interpret_effect_size)
)
```

## Quality Assurance Checklist

### Before Analysis

- [ ] **Setup system working**: `source("02_setup.R")` runs successfully
- [ ] **Method loaded**: `load_analysis_method()` for your analysis type
- [ ] **Effect size realistic**: Check against field guidelines  
- [ ] **Parameters complete**: Exactly 2 of (r_partial, n, power) specified

### During Analysis

- [ ] **Framework integration**: Use `framework_effect_size()` for conversions
- [ ] **Parameter validation**: Check for realistic ranges
- [ ] **Cross-method consistency**: Compare with similar analyses
- [ ] **Assumption planning**: Consider sample size for assumption validation

### After Analysis  

- [ ] **Results documentation**: Record effect size conversions
- [ ] **Sensitivity analysis**: Test alternative effect sizes
- [ ] **Method justification**: Ensure analysis matches statistical plan
- [ ] **Sample size rationale**: Document planning decisions

## Common Error Quick Reference

| **Error Message** | **Probable Cause** | **Quick Solution** |
|------------------|-------------------|-------------------|
| "could not find function X" | Method not loaded | `load_analysis_method("method_name")` |
| "Provide exactly two of..." | Wrong parameter count | Specify exactly 2 of: r_partial, n, power |
| "between -1 and 1" | Wrong effect size metric | Use `framework_effect_size()` for conversion |
| "Sample size must be >= X" | Insufficient sample | Increase n or reduce model complexity |
| "Analysis method not available" | File missing | Check `show_available_methods()` for alternatives |
| "Degrees of freedom" | Model too complex | Reduce n_predictors or increase n |

## Framework Best Practices Summary

### Setup and Loading

1. **Always use setup system**: `source("02_setup.R")` provides best interface
2. **Load methods explicitly**: `load_analysis_method()` for clarity
3. **Verify availability**: `show_available_methods()` before planning

### Effect Size Planning

1. **Use framework conversions**: `framework_effect_size()` for all conversions
2. **Apply conservative planning**: Framework discount factor prevents overpowering
3. **Reference field guidelines**: See `effect_size_guidelines.md` for appropriate ranges
4. **Document conversion decisions**: Track original metrics and framework values

### Analysis Execution

1. **Follow auto-detection pattern**: Provide exactly 2 of 3 core parameters
2. **Use framework integration**: `effect_input` and `effect_type` for automatic conversion
3. **Cross-validate methods**: Compare similar analyses using same r_partial
4. **Plan for assumptions**: See `assumption_validation_guide.md` for sample size requirements

### Integration and Documentation

1. **Literature synthesis**: Convert all effects to framework standard before meta-analysis
2. **Cross-method comparison**: Leverage unified r_partial for efficiency assessment
3. **Complete documentation**: Record all conversions, assumptions, and planning decisions
4. **Quality assurance**: Use validation workflows for critical studies

The unified framework prevents most common power analysis errors through consistent interfaces, realistic effect size conversion, and cross-method integration. When issues arise, they typically involve setup problems, unrealistic effect sizes, or parameter specification errors that this guide addresses systematically.
