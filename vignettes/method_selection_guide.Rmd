---
title: "Method Selection Guide"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Method Selection Guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(rphbPower)
```

Comprehensive guidance for choosing the optimal power analysis method within the unified partial correlation framework.

## Framework Overview

All power analysis methods in this package use **partial correlations** as the standardized effect size metric, enabling direct comparison across correlation, regression, mediation, SEM, multilevel, and longitudinal approaches. This unified framework simplifies method selection while maintaining statistical accuracy.

**Key Advantages:**

- **Direct comparison**: Same effect size (r_partial) across all methods
- **Conservative planning**: Built-in 0.75 discount factor for realistic estimates
- **Auto-detection**: Provide any 2 of (effect size, sample size, power) - calculates the third
- **Unified interface**: Consistent function patterns across all analysis types

## Quick Method Selection

### By Research Design

| **Research Question** | **Primary Method** | **Key Function** | **Notes** |
|----------------------|-------------------|------------------|-----------|
| Simple relationship between X and Y | Correlation | `correlation_power()` | Bivariate associations |
| X predicts Y (multiple predictors) | Linear Regression | `linear_regression_power()` | Most common choice |
| Binary outcome prediction | Logistic Regression | `logistic_regression_power()` | Yes/no, success/failure; Handles multiple predictors |
| Within-subjects design (2 waves) | Repeated Measures | `repeated_measures_power()` | Pre-post, two conditions |
| Clustered/nested data | Multilevel Models | `mixed_models_power()` | **Note: L1 engine is simulation-based; only solves for power** |
| Causal pathway testing | Mediation | `mediation_regression_power()` | X → M → Y mechanisms; fast analytical engine |
| Complex structural models | SEM | `sem_direct_effects_power()` | Latent variables, accounts for reliability |
| Panel/longitudinal data | Fixed Effects | `fixed_effects_power()` | Within-person effects over many time points |
| Reciprocal causation | Cross-Lagged Panel | `cross_lagged_panel_power()` | X ⟷ Y over time; uses regression engine |
| Robust to outliers | Nonparametric | `wilcoxon_signed_rank_power()` | Distribution-free, uses paired t-test engine |

### By Data Characteristics

| **Data Type** | **Design Features** | **Recommended Method** |
|---------------|-------------------|----------------------|
| **Continuous outcome** | Single predictor | Linear regression |
| **Continuous outcome** | Multiple predictors | Linear regression |
| **Continuous outcome** | Repeated measures | Repeated measures |
| **Continuous outcome** | Clustered structure | Mixed models |
| **Binary outcome** | Any design | Logistic regression |
| **Ordinal/skewed** | Robust alternative needed | Nonparametric |
| **Longitudinal** | Many time points | Fixed effects |
| **Longitudinal** | Reciprocal effects | Cross-lagged panel |

## Framework Method Comparison

### Same Effect Size, Different Methods

The unified framework enables direct efficiency comparison:

```{r efficiency-comparison}
# Same research question: r_partial = 0.25
effect_size <- 0.25

# Compare sample size requirements
corr_n <- correlation_power(r_partial = effect_size, power = 0.8)$n
reg_n <- linear_regression_power(r_partial = effect_size, power = 0.8, n_predictors = 1)$n
rm_n <- repeated_measures_power(r_partial = effect_size, power = 0.8, n_timepoints = 2)$n

# Results show relative efficiency
data.frame(
  Method = c("Correlation", "Linear Regression", "Repeated Measures"),
  Required_N = c(corr_n, reg_n, rm_n),
  Efficiency = c("Baseline", "Similar", "Much better")
)
```


## Detailed Method Specifications

### Linear Regression

**Best for**: Most research questions with continuous outcomes

```{r linear-regression}
# Framework integration examples
linear_regression_power(r_partial = 0.25, power = 0.8, n_predictors = 1)     # Calculate n
linear_regression_power(r_partial = 0.25, n = 120, n_predictors = 3)         # Calculate power
linear_regression_power(n = 150, power = 0.8, n_predictors = 2)              # Calculate effect

# Framework effect input with automatic conversion
linear_regression_power(
  effect_input = 0.4,          # Cohen's d from literature
  effect_type = "d",           # Automatic conversion to r_partial
  power = 0.8,
  n_predictors = 3
)
```


**When to use:**

- Continuous outcome variable
- One or more predictors (continuous or categorical)
- Independent observations
- Normal residuals acceptable

**Sample size considerations:**

- More predictors require larger samples
- Framework accounts for model complexity automatically
- Conservative discount factor improves replication success

### Logistic Regression

**Best for**: Binary outcomes (yes/no, success/failure, admitted/rejected).

```{r log-regression}
# Recommended workflow: Use an odds ratio (OR) and specify all predictors.
# This example calculates the power for a given sample size.

logistic_regression_power(
  effect_input = 2.0,      # Expected Odds Ratio
  effect_type = "or",      # Specify 'or' as the effect type
  n = 350,                 # Provide N to solve for power
  n_predictors = 4         # Correctly accounts for all predictors
)
```

**Key considerations:**

- **Engine**: Power is calculated using a robust **Monte Carlo Simulation** to ensure high accuracy, especially in models with multiple predictors.
- **Recommended Effect Size**: The **odds ratio (OR)** is the most natural effect size for this method. You should provide it directly via `effect_input`.
- **Model Complexity**: The `n_predictors` argument is now **fully supported** and essential for accurate planning.

### Repeated Measures

**Best for**: Within-subjects designs, especially pre-post comparisons

```{r repeated-measures}
# The engine uses a paired t-test, ideal for 2 timepoints.
repeated_measures_power(
  r_partial = 0.25,
  power = 0.8,
  n_timepoints = 2,
  correlation_between_measures = 0.6
)
```


**Advantages:**

- Controls for individual differences, leading to higher statistical power
- Requires smaller samples than equivalent between-subjects designs

**Key parameters & Engine:**

- The calculation engine is now the pwr.t.test function for a paired t-test, which is most appropriate for two timepoints (e.g., pre-test vs. post-test)
- The correlation_between_measures argument is preserved in the function signature for context but is not used by the new calculation engine

### Mixed Models

**Best for**: Clustered, nested, or hierarchical data (e.g., students in classrooms, patients in hospitals)

```{r mixed-models-new}
# Calculate power for a Level-1 (within-group) effect
mixed_models_power(
  r_partial = 0.25,
  n_groups = 50,
  n_per_group = 25,
  icc = 0.15,
  test_level = "level1"
)
```

**Key considerations:**

- **Test Level**: You must specify whether you are testing a level1 (within-group) or level2 (between-group) effect.
- **Engine**: The engine for Level-2 effects is analytical (using pwr). The engine for Level-1 effects is a robust Monte Carlo simulation.
- **Limitations**: Because it uses simulation, the Level-1 engine can only solve for power.

### Mediation Analysis

**Best for**: Testing causal pathways (X → M → Y)

```{r mediation-analysis}
# Regression-based mediation using the fast analytical engine
mediation_regression_power(
  r_a = 0.30,                  # X → Mediator
  r_b = 0.35,                  # Mediator → Y (controlling X)
  power = 0.8,
  n = NULL
)

# SEM-based mediation, accounting for measurement reliability
mediation_sem_power(
  r_a = 0.25,
  r_b = 0.40,
  power = 0.8,
  measurement_reliability = 0.9
)
```


**Method selection:**

- **Regression-based**: Use for simple mediation with observed (not latent) variables. The engine is a fast analytical formula (Aroian test)
- **SEM-based**: Use when you have latent variables and must account for the attenuating effect of measurement reliability

### Cross-Lagged Panel Models

**Best for**: Reciprocal causation over time (X ↔ Y)

```{r cross-lagged-panel}
cross_lagged_panel_power(
  r_partial = 0.15,
  power = 0.8,
  n_waves = 3
)
```


**Design considerations:**

- **Engine**: Power is calculated by delegating to the linear_regression_power function, correctly assuming n_predictors = 2 (one stability path, one cross-lagged path)
- Requires a minimum of 3 waves for model identification
- High stability coefficients in the model will reduce power to detect the cross-lagged paths

### Fixed Effects Models

**Best for**: Panel data with many time points to control for stable, unobserved confounders

```{r fixed-effects}
fixed_effects_power(
  r_partial = 0.15,
  power = 0.8,
  n_periods = 8,
  n_units = NULL # <- Solving for number of units
)
```


**Advantages:**

- Estimates the effect of a predictor on an outcome within an individual or unit (e.g., person, firm) over time
- The calculation engine correctly uses degrees of freedom of (N_units * N_periods) - N_units - 1

### SEM Direct Effects

**Best for**: Estimating power for a single path in a complex structural model with latent variables

```{r sem-direct-effects}
sem_direct_effects_power(
  r_partial = 0.25,
  power = 0.8,
  n_predictors = 4,
  measurement_reliability = 0.90
)
```


**Sample size considerations:**

- **Measurement Reliability**: This is a critical parameter. The latent r_partial is attenuated by the reliability of your measures. Lower reliability drastically reduces power and requires a much larger sample size
- This function provides a fast approximation; a full Monte Carlo simulation is recommended for final grant proposals

## Advanced Method Selection

### Efficiency Comparisons

The framework enables sophisticated efficiency analysis:

```{r efficiency-comparisons}
# Same research question, different designs
research_effect <- 0.20

# Between-subjects (independent samples)
between_n <- linear_regression_power(r_partial = research_effect, power = 0.8, n_predictors = 1)$n

# Within-subjects (repeated measures)  
within_n <- repeated_measures_power(r_partial = research_effect, power = 0.8, n_timepoints = 2)$n

# Mixed design efficiency
# NOTE: The L1 engine only solves for power. We test the power for N=200 (20 groups of 10).
mixed_power <- mixed_models_power(r_partial = research_effect, n_groups = 20, n_per_group = 10, test_level = "level1")$power

# Efficiency comparison
cat(paste("N for 80% power (Between-subjects):", between_n))
cat(paste("\nN for 80% power (Within-subjects):", within_n))
cat(paste("\nPower for N=200 (Mixed Model L1):", round(mixed_power, 2)))
```


## Method Selection Decision Tree

### Step 1: Outcome Variable Type

```
Outcome Variable
├── Continuous → Consider regression methods
├── Binary → Use logistic_regression_power()
├── Ordinal/Skewed → Consider wilcoxon_signed_rank_power()
└── Count/Rate → Use Poisson regression (future development)
```

### Step 2: Design Structure

For Continuous Outcomes:

```
├── Independent observations
│   ├── Single predictor → linear_regression_power()
│   └── Multiple predictors → linear_regression_power()
├── Repeated measures (2 waves) → repeated_measures_power()
├── Clustered/nested → mixed_models_power()
├── Longitudinal panel → fixed_effects_power()
└── Reciprocal causation → cross_lagged_panel_power()
```

### Step 3: Research Question Type

Specific Questions:

```
├── Association/prediction → regression methods
├── Causal mechanism → mediation_regression_power()
├── Complex theory → sem_direct_effects_power()
└── Simple relationship → correlation_power()
```

## Framework Best Practices

### Effect Size Selection Strategy

```{r effect-size-selection}
# Literature integration across studies with mixed metrics
literature_effects <- c(0.4, 0.12, 0.08)  # d, R², f²
effect_types <- c("d", "r_squared", "f2")

# Convert all to framework standard
r_values <- mapply(framework_effect_size, literature_effects, effect_types, 
                   MoreArgs = list(apply_discount = TRUE))

# Use meta-analytic average
meta_r <- mean(r_values)

# Apply to any method
linear_regression_power(r_partial = meta_r, power = 0.8, n_predictors = 3)
mediation_regression_power(r_a = meta_r, r_b = meta_r, power = 0.8, n = NULL)
```


### Cross-Method Validation

```{r cross-method-validation}
# Validate method choice by comparing alternatives
base_effect <- 0.25

# Method 1: Simple regression
simple_result <- linear_regression_power(r_partial = base_effect, power = 0.8, n_predictors = 1)

# Method 2: With covariates  
complex_result <- linear_regression_power(r_partial = base_effect, power = 0.8, n_predictors = 4)

# Method 3: Repeated measures alternative
within_result <- repeated_measures_power(r_partial = base_effect, power = 0.8, n_timepoints = 2)

# Compare sample size requirements
data.frame(
  Method = c("Simple", "With Covariates", "Repeated Measures"),
  Sample_Size = c(simple_result$n, complex_result$n, within_result$n),
  Relative_Efficiency = c(1.0, complex_result$n/simple_result$n, within_result$n/simple_result$n)
)
```


## Common Selection Errors

### ❌ Frequent Mistakes

- **Ignoring clustering**: Using simple regression when data is nested
- **Wrong effect size metric**: Using zero-order correlations for a regression model with multiple predictors without accounting for the other variables
- **Overcomplicated methods**: Using SEM when a simpler regression model suffices
- **Ignoring dependencies**: Using an independent samples test for repeated measures data

### ✅ Framework Solutions

- **Unified effect sizes**: All methods use comparable r_partial values
- **Automatic conversions**: Framework handles effect size transformations
- **Conservative planning**: Built-in discount factor prevents underpowering
- **Method comparison**: Easy to compare alternatives with the same effect size

## Field-Specific Guidance

### Psychology Research

- **Common methods**: Linear regression, repeated measures, mediation
- **Typical effects**: r_partial = 0.15-0.35
- **Framework advantages**: Direct comparison across experimental and correlational studies

### Educational Research

- **Common methods**: Mixed models, mediation, longitudinal
- **Typical effects**: r_partial = 0.20-0.40
- **Framework advantages**: Handles nested data (students in classrooms) naturally

### Medical Research

- **Common methods**: Logistic regression, mixed models, repeated measures
- **Typical effects**: r_partial = 0.10-0.25
- **Framework advantages**: Conservative planning crucial for clinical trials

### Business Research

- **Common methods**: Linear regression, SEM, panel data
- **Typical effects**: r_partial = 0.15-0.30
- **Framework advantages**: Complex structural models with unified effect sizes

## Method Selection Checklist

Before finalizing your choice:

### Research Design Match

- Method matches planned statistical analysis
- Accounts for all design features (clustering, repeated measures, etc.)
- Appropriate for outcome variable type

### Framework Integration

- Effect size converted to r_partial using framework functions
- Conservative discount factor applied
- Sample size accounts for model complexity

### Practical Considerations

- Sample size realistic for resources
- Effect size appropriate for field
- Power adequate for research goals

### Validation

- Cross-check with alternative methods using framework
- Sensitivity analysis with different effect sizes
- Documentation matches analysis plan

The unified framework simplifies method selection while ensuring statistical rigor and realistic power analysis for successful research planning.
