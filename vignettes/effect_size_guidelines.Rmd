---
title: "Effect Size Guidelines"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Effect Size Guidelines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(rphbPower)
```

Comprehensive guidelines for selecting appropriate effect sizes within the unified partial correlation framework with conservative planning and cross-method consistency.

## Framework Overview

The unified framework uses **partial correlations** as the standardized effect size metric across all analysis types, enabling direct comparison between correlation, regression, mediation, SEM, multilevel, and longitudinal approaches.

**Key Features:**

- **Unified metric**: All analyses use partial correlations (r_partial)
- **Conservative planning**: Built-in 0.75 discount factor for realistic estimates
- **Cross-method consistency**: Same effect sizes enable direct comparison across methods
- **Framework integration**: Automatic conversion from Cohen's d, f², R², eta-squared
- **Validated engines**: All calculation engines have been corrected and validated to ensure accurate power analysis

## Universal Effect Size Interpretation

### Partial Correlations (Framework Standard)

| **Magnitude** | **|r_partial|** | **Interpretation** | **Variance Explained** |
|---------------|-----------------|-------------------|----------------------|
| **Negligible** | < 0.10 | Practically meaningless | < 1% |
| **Small** | 0.10 - 0.29 | Meaningful in context | 1% - 8% |
| **Medium** | 0.30 - 0.49 | Clearly meaningful | 9% - 24% |
| **Large** | ≥ 0.50 | Very strong relationship | ≥ 25% |

### Cohen's Conventional Benchmarks (Framework Updated)

- **Small**: r = 0.10 (Cohen's d ≈ 0.20)
- **Medium**: r = 0.30 (Cohen's d ≈ 0.60)  
- **Large**: r = 0.50 (Cohen's d ≈ 1.11)

**Framework Reality**: Most research achieves small-to-medium effects (r = 0.10-0.30) after conservative discount factor application.

### Odds Ratios (for Logistic Regression)

The odds ratio (OR) is the most common effect size in logistic regression. An OR of **1.0 indicates no effect**. The magnitude of the effect is how far the OR is from 1.0 in either direction.

| **Magnitude** | **Odds Ratio (OR) for Risk or Protective Effect** | **Interpretation** |
|---------------|---------------------------------------------------|--------------------------------------------|
| **Small** | **1.50 or 0.67** | A small but potentially meaningful change in odds. |
| **Medium** | **3.45 or 0.29** | A substantial change in odds. |
| **Large** | **9.00 or 0.11** | A very strong, decisive change in odds. |

*Note: Benchmarks are based on conventions mapping to Cohen's d. The values less than 1 are the inverses of the values greater than 1 (e.g., 1/1.50 ≈ 0.67), representing effects of the same strength in the opposite direction.*

**Framework Guidance**: You can now provide the odds ratio directly to the `logistic_regression_power` function by setting `effect_type = "or"`. The function correctly handles values both greater and less than 1.

## Field-Specific Guidelines

### Psychology and Social Sciences

| **Research Area** | **Small** | **Medium** | **Large** | **Framework Guidance** |
|-------------------|-----------|------------|-----------|----------------------|
| **Clinical interventions** | 0.15 | 0.30 | 0.50 | Treatment vs. control effects |
| **Personality-behavior** | 0.10 | 0.25 | 0.40 | Individual differences research |
| **Social psychology** | 0.15 | 0.30 | 0.45 | Experimental manipulations |
| **Cognitive psychology** | 0.20 | 0.35 | 0.55 | Laboratory-controlled effects |

**Psychology Planning**: Expect r = 0.15-0.25 after framework discount for most applied research.

### Education Research

| **Research Area** | **Small** | **Medium** | **Large** | **Framework Guidance** |
|-------------------|-----------|------------|-----------|----------------------|
| **Academic achievement** | 0.10 | 0.25 | 0.40 | Student outcome measures |
| **Intervention programs** | 0.15 | 0.30 | 0.45 | Educational treatments |
| **Teacher effectiveness** | 0.12 | 0.28 | 0.42 | Classroom-level effects |
| **School factors** | 0.08 | 0.20 | 0.35 | Institutional influences |

### Health and Medical Research

| **Research Area** | **Small** | **Medium** | **Large** | **Framework Guidance** |
|-------------------|-----------|------------|-----------|----------------------|
| **Behavioral health** | 0.15 | 0.30 | 0.50 | Lifestyle interventions |
| **Risk factors** | 0.10 | 0.25 | 0.40 | Disease predictors |
| **Treatment efficacy** | 0.20 | 0.35 | 0.55 | Clinical trials |
| **Quality of life** | 0.12 | 0.28 | 0.45 | Patient-reported outcomes |

### Business and Organizational Research

| **Research Area** | **Small** | **Medium** | **Large** | **Framework Guidance** |
|-------------------|-----------|------------|-----------|----------------------|
| **Employee performance** | 0.12 | 0.28 | 0.45 | Individual productivity |
| **Training effectiveness** | 0.15 | 0.32 | 0.50 | Learning interventions |
| **Customer satisfaction** | 0.10 | 0.25 | 0.40 | Service quality measures |
| **Leadership impact** | 0.08 | 0.22 | 0.38 | Management effects |

## Framework Integration Examples

### Converting Literature Effects

```{r convert-lit-effects}
# Convert Cohen's d from literature
literature_d <- 0.45
r_framework <- framework_effect_size(literature_d, "d", apply_discount = TRUE)
# Result: d = 0.45 → r ≈ 0.17 (after 0.75 discount)

# Convert R² from meta-analysis  
meta_r_squared <- 0.12
r_framework <- framework_effect_size(meta_r_squared, "r_squared", apply_discount = TRUE)
# Result: R² = 0.12 → r ≈ 0.26 (after discount)

# Use in any framework analysis
result <- linear_regression_power(r_partial = r_framework, power = 0.8, n_predictors = 3)
# Typical sample size: 300-500 participants depending on exact conversion
```


## Analysis-Specific Considerations

### Correlation Analysis

- **Zero-order**: Your effect size estimate is the direct input for r_partial
- **Typical range**: r = 0.10-0.40 in most research domains
- **Framework advantage**: Simple 1:1 correspondence with other methods for single-predictor models

### Linear Regression
- **Single predictor**: Sample size requirements are very similar to a standard correlation.
- **Multiple predictors**: The `r_partial` represents the unique effect of one predictor controlling for all others. Expect to need a slightly larger sample size as more predictors are added, which you can model with the `n_predictors` argument.

### Logistic Regression
- **Recommended Effect Size**: The **odds ratio (OR)** is the most natural effect size. You can provide it directly using `effect_input = YOUR_OR` and `effect_type = "or"`.
- **Engine**: Power is calculated using a robust **Monte Carlo Simulation** engine to ensure high accuracy, especially in models with multiple predictors.
- **Model Complexity**: The `n_predictors` argument is fully supported, allowing you to accurately plan for models with covariates.

### Mediation Analysis (Regression-based)

- **Individual paths**: The effect sizes are the r_a (X→M) and r_b (M→Y|X) path coefficients
- **Indirect effects**: The power is for the indirect effect (a*b), which is typically small (r = 0.02-0.15)
- **Framework Engine**: The calculation is performed by a fast analytical engine using the Aroian formula for the Sobel test, replacing the previous slow simulation method

### Longitudinal Analysis

- **Cross-Lagged Effects**: Often small (r = 0.05-0.20). The power calculation is now delegated to the linear_regression_power function, correctly treating it as a regression with 2 predictors (the cross-lagged path and the stability path)
- **Repeated Measures (2 waves)**: The calculation uses a paired t-test engine (pwr.t.test). The r_partial is converted to the equivalent Cohen's d for the analysis. Ideal for pre-post designs
- **Fixed Effects (Multiple waves)**: For analyzing within-person effects over time. The engine uses a t-test with degrees of freedom adjusted for the number of units and time periods (N*T - N - k)

### SEM and Multilevel Models

- **SEM Direct Effects**: Similar to regression, but you must account for measurement reliability. The framework attenuates the latent r_partial by the square root of the measurement_reliability argument before calculating power. Lower reliability requires a much larger sample size
- **Multilevel (Mixed) Models**: The framework distinguishes between Level-1 (within-group) and Level-2 (between-group) effects via the test_level argument
- **Intraclass Correlation (ICC)**: The icc parameter is critical and dramatically impacts power for Level-1 effects
- **Framework Engine**: Calculations are delegated to the WebPower and pwr packages

## Effect Size Planning Workflow

### Cross-Method Sample Size Comparison

Use same r_partial across different analyses to compare sample size requirements:

```{r cross-method-sample-size}

planning_r <- 0.25  # From literature integration

# Compare sample size requirements across methods
correlation_n <- correlation_power(r_partial = planning_r, power = 0.8)$n
regression_n <- linear_regression_power(r_partial = planning_r, power = 0.8, n_predictors = 3)$n
mediation_n <- mediation_regression_power(r_a = planning_r, r_b = planning_r, power = 0.8, n = NULL)$n

# Typical results for r = 0.25:
# Correlation: ~122
# Regression (3 pred): ~135  
# Mediation: ~180-200
```


## Common Planning Mistakes and Solutions

### ❌ Avoid These Critical Errors

- **Using Cohen's conventions without field context**: Effects vary dramatically by domain
- **Expecting large effects in most research domains**: Large effects (r > 0.50) are rare
- **Ignoring measurement reliability effects**: Poor measurement attenuates observed effects. The framework's SEM functions now help you model this directly with the measurement_reliability parameter
- **Forgetting about restriction of range**: Sampling constraints reduce effect sizes
- **Using optimistic pilot study effects without discount**: Pilot effects often don't replicate
- **Underestimating model complexity impact**: Multiple predictors substantially increase sample requirements

### ✅ Framework Best Practices

- **Research field-specific effect size norms**: Use domain-appropriate expectations
- **Plan for small-to-medium effects in most domains**: r = 0.10-0.30 after discount is typical
- **Trust framework discount factor for conservative planning**: 0.75 discount prevents underpowering
- **Account for measurement quality and design factors**: Use the measurement_reliability and icc parameters when appropriate
- **Apply unified r_partial for cross-method consistency**: Enables direct comparison across analyses
- **Use actual framework calculations**: Avoid rules of thumb, use verified mathematical functions

## Framework Integration Benefits

The unified partial correlation framework enables:

- **Cross-method comparison**: Same effect size metric across all analyses
- **Conservative planning**: Built-in discount factor prevents underpowered studies
- **Literature integration**: Convert any effect size type to common metric
- **Validated and Fast Engines**: Flexible and Validated Engines: Uses fast analytical formulas where possible and robust Monte Carlo simulations for complex models (like logistic regression) where analytical solutions are less accurate.
- **Mathematical precision**: Validated calculations ensure accurate power analysis

## Validation Status

✅ **Field-Specific Guidelines**: Updated with framework discount considerations  
✅ **Conversion Accuracy**: All effect size transformations verified  

✅ **Conservative Planning**: Consistent application of 0.75 discount factor  

✅ **Cross-Method Integration**: Unified partial correlations enable precise comparisons  

✅ **Validated Mathematical Foundation**: Based on delegation to trusted packages and corrected analytical formulas  

The framework ensures methodological consistency while providing practical effect size guidance grounded in empirical research across diverse domains, enabling researchers to make informed decisions about study planning with realistic and defensible effect size estimates that account for the complexities of real-world research.
